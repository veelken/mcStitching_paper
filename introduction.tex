\section{Introduction}
\label{sec:introduction}

Monte Carlo (MC) simulations~\cite{Kroese2014WhyTM,dunn2011exploring} are used for a plethora of different purposes in contemporary high-energy physics (HEP) experiments.
Applications for experiments currently in operation include detector calibration; optimization of analysis techniques, including the training of machine learning algorithms;
the modelling of backgrounds, as well as the modelling of signal acceptance and efficiency.
Besides, MC simulations are extensively used for detector development and for estimating the physics reach of experiments that are presently in construction or planned in the future.
When using MC simulations for the purpose of modelling background contributions,
the production of sufficiently large MC samples often poses a material challenge in terms of the computing resources required to produce and store such samples.

This is especially true for experiments at the CERN Large Hadron Collider (LHC)~\cite{Bruning:2004ej,Buning:2004wk,Benedikt:2004wm},
firstly due to the large cross section of proton-proton ($\Pp\Pp$) collisions and secondly due to the large luminosity delivered by the LHC.
We refer to a single $\Pp\Pp$ collision as an {\em event}.
The number of events, $N_{\data}$, produced within a given interval of time 
is given by the product of the $\Pp\Pp$ scattering cross section, $\sigma$, and of the integrated luminosity, $L$, that the LHC has delivered during this time:
$N_{\data} = \sigma \times L$.
The inelastic $\Pp\Pp$ scattering cross section at the present LHC center-of-mass energy of $\sqrt{s}=13$~\TeV amounts to $\approx 75$~mb~\cite{Aaboud:2016mmw,Sirunyan:2018nqx},
while the integrated luminosities recorded at $\sqrt{s}=13$~\TeV by the ATLAS and CMS experiments amount to $\approx 140$~fb$^{-1}$~\cite{ATLAS-CONF-2019-021,LUM-17-001,LUM-17-004,LUM-18-002}.
Thus, $N_{\data} \approx 10^{16}$ inelastic $\Pp\Pp$ scattering events occurred in each of the two experiments during this time.

In order to render the statistical uncertainties on background estimates obtained from the MC simulation small compared to the statistical uncertainties on the data,
MC samples of size larger than $N_{\data}$ are needed, ideally $N_{\mc} \gtrsim 10 \, N_{\data}$,
where the size of the MC sample is denoted by the symbol $N_{\mc}$.
Such large MC sample are prohibitive to produce.
The solution to this apparent dilemma is to restrict the production of MC samples to the processes most relevant for physics analyses,
which typically focus on events that contain either charged leptons, jets of high transverse momentum ($\pT$), or high $\pT$ neutrinos,
where the latter is indicated by the presence of large missing transverse momentum in the event.
The cross sections of these processes are orders of magnitude lower compared to the inelastic $\Pp\Pp$ scattering cross section~\footnote{ 
See Ref.~\cite{StandardModelCrossSections} for a summary of Standard Model cross sections relevant for physics analyses at the LHC.}.
Elaborate MC production schemes are in common use at the LHC.
These schemes typically restrict the set of MC samples to those processes most relevant for physics analyses
and furthermore limit the size of each MC sample as much as possible, 
while keeping the statistical uncertainties on background estimates at an acceptable level.

The production of MC samples of sufficient size is of particular importance in searches for new physics.
In these searches, potential signals are typically expected to be small
compared to the background contributions arising from established Standard Model (SM) processes
and the presence or absence of a signal may in fact be obscured by the statistical uncertainties on the background estimate,
unless MC samples of adequate size are available to model the main background processes~\footnote{
The presence of large signals is often already excluded by the results of previous searches, based on a smaller dataset.}.
Searches for new physics are often performed in phase-space (PS) regions that are atypical for background processes,
as only in these regions the ratio $S/\sqrt{B}$ is sufficiently large to allow for the discovery of potential signals.
The MC production schemes employed by the ATLAS and CMS experiments utilize the fact 
that typically only a small percentage of the background populates the regions of PS most relevant for physics analyses.
A typical strategy is to divide the PS into multiple regions and to produce separate MC samples for each region,
with the size of these MC samples being adapted to the needs of physics analyses.

The sets of MC samples produced following this strategy often overlap in PS,
as a consequence of different schemes being used for dividing the PS into distinct regions by different physics analyses.
For example, one set of MC samples may divide the PS based on the number of jets, 
whereas another set of MC samples may divide the PS based on $\HT$, the scalar sum in $\pT$ of charged leptons plus jets in the event.
In this paper, we present a procedure for combining MC samples in an ``optimal'' way,
where optimal refers to yielding the lowest statistical uncertainty on the background estimate that can be achieved when combining given sets of MC samples.
Our formalism handles the case that different MC samples may use different schemes to divide the PS into distinct regions,
thereby allowing to use all available MC samples, regardless of any arbitrary overlap in PS between these samples.
The overlap between MC samples in PS is accounted for by applying appropriately chosen weights to simulated events.
We refer to this procedure as ``stitching''.

The structure of this paper is as follows:
the formalism for computing the stitching weights is developed in Section~\ref{sec:stitching_weights}.
In Section~\ref{sec:examples}, we present examples for applying the formalism to $\Pp\Pp$ collisions at the LHC.
The paper concludes with a summary in Section~\ref{sec:summary}.
