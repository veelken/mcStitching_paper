\section{Introduction}
\label{sec:introduction}

Monte Carlo (MC) simulations~\cite{Kroese2014WhyTM,dunn2011exploring} are used for a plethora of different purposes in contemporary high-energy physics (HEP) experiments.
Applications for experiments currently in operation include detector calibration; optimization of analysis techniques, including the training of machine learning algorithms;
the modelling of backgrounds, as well as the modelling of signal acceptance and efficiency.
Besides, MC simulations are extensively used for detector development and for estimating the physics reach of experiments that are presently in construction or planned in the future.

The production of MC samples containing a sufficient number of events often poses a material challenge 
in terms of the computing resources required to produce and store such samples~\cite{HSFPhysicsEventGeneratorWG:2020gxw}.
This is especially true for experiments at the CERN Large Hadron Collider (LHC)~\cite{Bruning:2004ej,Buning:2004wk,Benedikt:2004wm},
firstly due to the large cross section for proton-proton ($\Pp\Pp$) scattering and secondly due to the large luminosity delivered by the LHC.

The number of $\Pp\Pp$ scattering interactions, $N_{\data}$, that occur within a given interval of time 
is given by the product of the $\Pp\Pp$ scattering cross section, $\sigma$, and of the integrated luminosity, $L$, that the LHC has delivered during this time:
$N_{\data} = \sigma \, L$.
We refer to the ensemble of $\Pp\Pp$ scattering interactions that occur within the same crossing of the proton bunches as an ``event''.
The interaction with the highest momentum exchange between the protons is referred to as the ``hard-scatter'' interaction,
and the remaining interactions are referred to as ``pileup''.
The inelastic $\Pp\Pp$ scattering cross section at the center-of-mass energy of $\sqrt{s}=13$~\TeV, the energy achieved during the recently completed Run $2$ of the LHC (in the period $2015$-$2018$),
amounts to $\approx 75$~mb~\cite{Aaboud:2016mmw,Sirunyan:2018nqx}.
The $\Pp\Pp$ scattering data recorded by the ATLAS and CMS experiments during LHC Run $2$ 
amounts to an integrated luminosity of $\approx 140\fbinv$ per experiment~\cite{ATLAS-CONF-2019-021,LUM-17-001,LUM-17-004,LUM-18-002}.
Thus, $N_{\data} \approx 10^{16}$ inelastic $\Pp\Pp$ scattering interactions occurred in each of the two experiments during this time.
Ideally, one would want the number of simulated events to be higher than the number of events in the data,
such that the statistical uncertainties on the MC simulation are small compared to the statistical uncertainties on the data.
The production of such large MC samples is clearly prohibitive, however.

Even if one restricts the production of MC samples to processes with a cross section that is significantly smaller than the inelastic $\Pp\Pp$ scattering cross section,
such as Drell-Yan (DY) production, the production of $\PW$ bosons ($\PW$+jets), and the production of top quark pairs ($\Ptop\APtop$+jets),
the production of MC samples containing a sufficient number of events to allow for a meaningful comparison with the data represents a formidable challenge.
The DY, $\PW$+jets, and $\Ptop\APtop$+jets production processes are used for detector calibration and Standard Model (SM) precision measurements.
They also constitute relevant backgrounds to searches for physics beyond the SM.
Their cross sections amount to $6.08$~nb for DY production, $61.5$~nb for $\PW$+jets production,
and $832$~pb for $\Ptop\APtop$+jets production~\cite{Melnikov:2006kv,Li:2012wna,Czakon:2011xx}~\footnote{
  The quoted cross sections refer to, respectively, DY production of lepton (electron, muon, and $\Pgt$) pairs of mass $> 50$~\GeV,
  $\PW$+jets production with subsequent leptonic decay of the $\PW$ boson,
  and to the pair production of top quarks of mass $172.5$~\GeV.
}.
The ATLAS and CMS experiments would each need to produce MC samples containing $840$ million DY, $8.61$ billion $\PW$+jets, and $116$ million $\Ptop\APtop$+jets events
in order to reduce the statistical uncertainties on the MC simulation to the same level as the uncertainties on the LHC Run $2$ data.

In order to mitigate the effect of limited computing resources, both experiments employ sophisticated strategies for the production of MC samples.
A common feature of these strategies is to vary the expenditure of computing resources across phase space (PS),
depending on the needs of physics analyses.
When searching for new physics, for example, it is important to produce sufficiently many events in the tails of distributions,
as otherwise potential signals may be obscured by the statistical uncertainties on the SM background.

Different mechanisms for adapting the expenditure of computing resources to the needs of physics analyses have been proposed in the literature.
Modern MC programs (``generators'') such as \POWHEG~\cite{POWHEG1,POWHEG2,POWHEG3}, \MGvATNLO~\cite{MGvATNLO}, \SHERPA~\cite{SHERPA}, \PYTHIA~\cite{PYTHIA}, and \HERWIG~\cite{HERWIG}
provide functionality that allows to adjust the number of events sampled in different regions of PS through user-defined weighting functions.
This approach has been used in Ref.~\cite{ATLAS:2021yza}.
An alternative approach is to partition the PS into distinct regions (``slices'') and to produce separate independent MC samples covering each slice.
Following Ref.~\cite{HSFPhysicsEventGeneratorWG:2020gxw}, we refer to the first approach as ``biasing'' and to the second one as ``slicing''.

In this paper, we focus on the case that MC samples have already been produced and present a method that makes optimal use of these samples,
where ``optimal'' refers to yielding the lowest statistical uncertainty on the signal or background estimate that is obtained from these samples.
The samples in general overlap in PS.
For example, one set of MC samples may partition the PS based on the number of jets, 
whereas another set of samples may partition the PS based on $\HT$, the scalar sum in $\pT$ of these jets.
Our method is general enough to handle arbitrary overlaps between these samples.
The overlap is accounted for by applying appropriately chosen weights to the simulated events.
We refer to the procedure as ``stitching''.
One useful feature of the stitching method is that it allows to increase the number of simulated events incrementally in certain regions of PS
in case these regions are not yet sufficiently populated by the existing MC samples.

In the following, we will assume that all MC samples that are subject to the stitching procedure 
have been produced with the same version of the MC program and consistent (\ie identical) settings 
for parton distribution functions, scale choices, parton-shower and underlying-event tunes, etc.
In case a given set of MC samples was produced with inconsistent settings,
the effect of the inconsistencies either need to be small (compared to \eg the systematic uncertainties) or the events need to be reweighted
to make all MC samples consistent prior to applying the stitching procedure.

Variants of the stitching procedure described in the first part of this manuscript have been used by the ATLAS and CMS experiments since LHC Run $1$,
but, to the best of our knowledge, have not been described in detail in a public document yet.
The formalism for the computation of stitching weights is detailed in Section~\ref{sec:stitching_weights}.
Concrete examples for using the formalism in physics analyses are given in Sections~\ref{sec:WJets_vs_Njet} and~\ref{sec:WJets_vs_Njet_and_HT}.
The examples characterize the use of the stitching procedure by the CMS experiment during LHC Runs $1$ and $2$.
They are chosen with the intention to provide a reference.
In Section~\ref{sec:examples_trigger_rate} we extend the stitching procedure to the case of estimating trigger rates at the High-Luminosity LHC (HL-LHC)~\cite{TDR_Phase2_LHC},
scheduled to start operation in $2027$.
The distinguishing feature between the applications of the stitching procedure described in Sections~\ref{sec:examples_background_yield} and~\ref{sec:examples_trigger_rate}
is that in the former (but not in the latter) the cross section of the process that is modeled by the MC simulation
is orders of magnitude smaller compared to the inelastic $\Pp\Pp$ scattering cross section.
In the former case one can make the simplifying assumption that the process of interest (the process modeled by the MC simulation) 
solely occurs in the hard-scatter interaction and not in pileup interactions.
For the purpose of estimating trigger rates, a relevant use case is that the hard-scatter interaction as well as the pileup interactions are inelastic $\Pp\Pp$ scattering interactions,
and the hard-scatter interaction is in fact indistinguishable from the pileup.
As described in detail in Section~\ref{sec:examples_trigger_rate},
we account for this indistinguishability by making suitable modifications to the formalism for the computation of stitching weights.
The modified stitching procedure detailed in Section~\ref{sec:examples_trigger_rate} has been used to estimate trigger rates 
for the HL-LHC upgrade technical design report of the CMS experiment~\cite{TDR-21-001}.
We conclude the paper with a summary in Section~\ref{sec:summary}.
